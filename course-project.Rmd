---
title: "Course Project - Practical Machine Learning"
author: "Stefanie Thiem"
date: "12 June 2015"
output:  
    html_document:  
        keep_md: true  
---

This project analyses data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. The data for this project is taken from the homepage http://groupware.les.inf.puc-rio.br/har. This website also contains further information on the data set. 

The goal is to predict how well the partipants performed the barbell lifts (categorized in classes A-E). We apply machine learning techniques to build a classifier which we then use to predict the class for the 20 provided test cases.

## Loading the Data 

First we load the data and the 20 test cases for this project. If the data file cannot be found in the working directory, we download if from the internet.
```{r, cache=TRUE}
if(!file.exists("./pml-training.csv")){
    URL = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
    download.file(URL, destfile = "./pml-training.csv", method="curl")
}
data <- read.csv("pml-training.csv")
```

```{r, cache=TRUE}
if(!file.exists("./pml-testing.csv")){
    URL = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
    download.file(URL, destfile="./pml-testing.csv", method="curl")
}
testCases <- read.csv("pml-testing.csv")
```

Additionally, we load all relevant libraries
```{r}
suppressWarnings(suppressMessages(library(caret)))
suppressMessages(library(rpart))
suppressMessages(library(rattle))
suppressMessages(library(dplyr))
```

## Cleaning and Preprocessing of Data

To allow the cross validation of the data, we split the data set into a training (60%) and testing set (40%). All machine learning steps are performed only on the training data set. We then evalute the accuracy and error rates on the testing data. Finally, the best model is used to classify the 20 provided test cases. 

```{r}
set.seed(1234)
inTraining = createDataPartition(data$classe, p=0.6, list=FALSE)
training = data[inTraining,]
testing = data[-inTraining,]
```

First, we check for covariates that have virtually no variablility and we remove these columns from the training data. We also remove the columns 1,2 and 4-6 because they don't add any further information to the system. This reduces the number of variables from 160 to 101.
```{r, cache=TRUE}
nzv <- nearZeroVar(training, saveMetrics=TRUE)
relevantVariables <- !nzv$nzv
training <- training[,relevantVariables]
rownames(training) <- NULL
training <- training[,c(-1,-3,-4,-5)]
```

The data set also contains many NA values. Determining the number of valid values for each variable actually reveals that many columns hardly contain any data (only 236 out of 11776 are valid entries). We will ignore these columns which leaves 55 remaining variables.
```{r}
number_nonNAs <- as.vector(apply(training, 2, function(x) length( which(!is.na(x) ))))
relevantVariables2 <- ifelse(number_nonNAs == nrow(training), TRUE, FALSE)
training <- training[,relevantVariables2]
```

## Machine Learning Algorithm

We now apply machine learning techniques to classify the data based on the variables of the cleaned data set. We would like to predict the outcome for the classe variable. We start by applying the method "rpart" which constructs a decission tree for the classifiation into groups A-E.
```{r, cache=TRUE}
modelFit <- train(classe ~ ., data = training, method = "rpart")
modelFit
fancyRpartPlot(modelFit$finalModel)
```

Unfortunately, the accuracy for this model is quite low with 56% on the training data. Applying it to the testing data would result in an even lower accuracy.

A more advances approach is to use Random Forests which is an ensemble learning method for classification and operates by constructing a multitude of decision trees. It also has the advantage of reducing overfitting which is a common draw back of decision trees. However, it comes at the cost of running time. We here restrict the algorithm to only do cross validation with 4 sub samples instead of more advanced methods.
```{r, cache=TRUE}
modelFit_randomForest <- suppressMessages(train(training$classe ~ ., method="rf", trControl=trainControl(method = "cv", number = 3), data=training))
modelFit_randomForest
```

This algorithm has a much higher accuracy and it is able to classify almost all cases of the training set correctly resulting in an accuracy of over 99%. No we check how it performs on the testing set. We find that it also has a very high accuracy of over 99% on the testing data.
```{r}
predictions <- suppressMessages(predict(modelFit_randomForest, newdata=testing))
confusionMatrix(predictions, testing$classe)
```

## Out of Sample Error

The **in sample error** is the error rate that we obtain for the same data set we used to build the predictor. For the Random Forest model the in sample error is 0.005.

The **out of sample error** is the error rate we get on a new data set, i.e., by using our model on the testing data. The Random Forest model also performes very well in this case and the out of sample error is 0.003. This error rate is what we use to classify the quality of the algorithm.

## Generate Data for Submitting

The final step is to apply our model to the 20 provided test cases.
```{r}
predictions <- predict(modelFit_randomForest, newdata=testCases)
predictions
```

To generate the output files with the predicted classifications we use:
```{r}
pml_write_files = function(x) {
    n = length(x)
    for(i in 1:n) {
        filename = paste0("problem_id_",i,".txt")
        write.table(x[i], file=filename, quote=FALSE, row.names=FALSE, col.names=FALSE)
    }
}
pml_write_files(predictions)
```

## Conclusion

The Random Forest algorithm showed superior performance for classifying the quality of the barbell lifts. We obtained over 99% accuracy on the testing set.
